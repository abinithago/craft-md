{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e74aead",
   "metadata": {},
   "source": [
    "This is a sequential implementation provided for readability. It is also possible to run this parallely (**recommended** for processing large number of case vignettes). The corresponding parallel script can be found in the file `run_conversation_withPE.py` or `run_conversation_withoutPE.py`. See `README.md` for instructions on how to run the scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77778ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import openai\n",
    "\n",
    "from src.utils import *\n",
    "from src.craft_md import get_doctor_prompt, get_patient_prompt, get_physical_exam_prompt, get_mcq_prompt, get_choices, get_all_choices, get_diagnosis_after_physical_exam_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad47e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API credentials\n",
    "# Replace this information with your OpenAI key and organization ID.\n",
    "openai_key = open(\"../keys/openai_key.txt\", \"r\")\n",
    "openai.api_key = openai_key.readlines()[0].strip()\n",
    "\n",
    "organization_id = open(\"../keys/rajpurkarlab_org_id.txt\", \"r\")\n",
    "openai.organization = organization_id.readlines()[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bbed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "dataset = pd.read_csv(\"./data/dataset_final.tsv\", sep = \"\\t\")\n",
    "all_choices = get_all_choices(path = './data/all_choices.txt')\n",
    "\n",
    "cases = [(dataset.loc[idx,\"case_id\"], \n",
    "             dataset.loc[idx,\"case_desc\"], \n",
    "             dataset.loc[idx,\"physical_exam\"], \n",
    "             get_choices(dataset,idx), \n",
    "             all_choices) for idx in range(dataset.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_case_withPE(case, save_path, gpt_model, num_runs = 10):\n",
    "    \n",
    "    mapping = {\"gpt-3.5\": call_gpt3_api, \"gpt-4\": call_gpt4_api}\n",
    "    \n",
    "    case_id, case_desc, exam, mcq_choices, mcq_many_choices = case\n",
    "    \n",
    "    doctor_prompt = get_doctor_prompt()\n",
    "    patient_prompt = get_patient_prompt(case_desc)\n",
    "    exam_prompt = get_physical_exam_prompt(exam)\n",
    "    \n",
    "    mcq_prompt = get_mcq_prompt(mcq_choices)\n",
    "    mcq_all_prompt = get_mcq_prompt(mcq_many_choices)\n",
    "        \n",
    "    stats = {case_id:{}}\n",
    "    j = 0\n",
    "\n",
    "    if os.path.exists(save_path): \n",
    "        stats = json.load(open(save_path, 'r'))\n",
    "            \n",
    "        # if key is already present, return without running again\n",
    "        if stats.get(case_id) != None:\n",
    "            while f'trial_{j}_doctor_responses' in stats[case_id]:\n",
    "                j+=1\n",
    "        else:\n",
    "            stats[case_id] = {}\n",
    "            \n",
    "    while j < num_runs:\n",
    "        conversation_history_doctor = [{\"role\": \"system\", \"content\": doctor_prompt}]\n",
    "        conversation_history_patient = [{\"role\": \"system\", \"content\": patient_prompt}]\n",
    "\n",
    "        # Multi-turn conversation without Physical Exam\n",
    "        while True:\n",
    "            # Patient talks\n",
    "            response_patient = mapping[gpt_model](conversation_history_patient, n_responses=1)\n",
    "\n",
    "            conversation_history_doctor.append({\"role\":\"user\",\n",
    "                                               \"content\":response_patient})\n",
    "            conversation_history_patient.append({\"role\":\"assistant\",\n",
    "                                               \"content\":response_patient})\n",
    "\n",
    "            # Doctor talks\n",
    "            response_doctor = mapping[gpt_model](conversation_history_doctor, n_responses=1)\n",
    "\n",
    "            conversation_history_doctor.append({\"role\":\"assistant\",\n",
    "                                               \"content\": response_doctor})\n",
    "            conversation_history_patient.append({\"role\":\"user\", \n",
    "                                                \"content\": response_doctor})\n",
    "\n",
    "            # Doctor arrives at a differential diagnosis\n",
    "            if (\"?\" not in response_doctor) or ('final diagnosis' in response_doctor.lower()):\n",
    "                break\n",
    "                \n",
    "        # Run multi-turn conversation (with Physical Exam) + FRQ\n",
    "        prompt = exam_prompt + get_diagnosis_after_physical_exam_prompt()\n",
    "        conversation_history_doctor.append({\"role\": \"system\", \"content\": prompt})\n",
    "        response_doctor = mapping[gpt_model](conversation_history_doctor, n_responses=1)\n",
    "        conversation_history_doctor.append({\"role\":\"assistant\",\"content\": response_doctor})\n",
    "        stats[case_id][f\"trial_{j}_doctor_responses_with_exam\"] = conversation_history_doctor\n",
    "        conversation_history_doctor = conversation_history_doctor[:-2]\n",
    "\n",
    "        # Run multi-turn conversation (with Physical Exam) + 4-choice MCQ\n",
    "        prompt = exam_prompt + mcq_prompt\n",
    "        conversation_history_doctor.append({\"role\": \"system\", \"content\": prompt})\n",
    "        response_doctor = mapping[gpt_model](conversation_history_doctor, n_responses=1)\n",
    "        stats[case_id][f\"trial_{j}_mcq_with_exam\"] = response_doctor\n",
    "        conversation_history_doctor.pop()\n",
    "\n",
    "        # Run multi-turn conversation (with Physical Exam) + many-choice MCQ\n",
    "        prompt = exam_prompt + mcq_all_prompt\n",
    "        conversation_history_doctor.append({\"role\": \"system\", \"content\": prompt})\n",
    "        response_doctor = mapping[gpt_model](conversation_history_doctor, n_responses=1)\n",
    "        stats[case_id][f\"trial_{j}_mcq_many_with_exam\"] = response_doctor\n",
    "        conversation_history_doctor.pop()\n",
    "        j += 1\n",
    " \n",
    "        json.dump(stats, open(save_path,\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144fb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_case_withoutPE(case, save_path, gpt_model, num_runs = 10):\n",
    "    \n",
    "    mapping = {\"gpt-3.5\": call_gpt3_api, \"gpt-4\": call_gpt4_api}\n",
    "    \n",
    "    case_id, case_desc, exam, mcq_choices, mcq_many_choices = case\n",
    "    \n",
    "    doctor_prompt = get_doctor_prompt()\n",
    "    patient_prompt = get_patient_prompt(case_desc)\n",
    "        \n",
    "    mcq_prompt = get_mcq_prompt(mcq_choices)\n",
    "    mcq_all_prompt = get_mcq_prompt(mcq_many_choices)\n",
    "    \n",
    "    stats = {case_id:{}}\n",
    "    j = 0\n",
    "    \n",
    "    if os.path.exists(save_path): \n",
    "        stats = json.load(open(save_path, 'r'))\n",
    "            \n",
    "        # if key is already present, return without running again\n",
    "        if stats.get(case_id) != None:\n",
    "            pass\n",
    "        else:\n",
    "            stats[case_id] = {}\n",
    "    \n",
    "    while j < num_runs:\n",
    "        conversation_history_doctor = [{\"role\": \"system\", \"content\": doctor_prompt}]\n",
    "        conversation_history_patient = [{\"role\": \"system\", \"content\": patient_prompt}]\n",
    "\n",
    "        while True:\n",
    "            # Patient talks\n",
    "            response_patient = mapping[gpt_model](conversation_history_patient, n_responses=1)\n",
    "\n",
    "            conversation_history_doctor.append({\"role\":\"user\",\n",
    "                                               \"content\":response_patient})\n",
    "            conversation_history_patient.append({\"role\":\"assistant\",\n",
    "                                               \"content\":response_patient})\n",
    "\n",
    "            # Doctor talks\n",
    "            response_doctor = mapping[gpt_model](conversation_history_doctor, n_responses=1)\n",
    "\n",
    "            conversation_history_doctor.append({\"role\":\"assistant\",\n",
    "                                               \"content\": response_doctor})\n",
    "            conversation_history_patient.append({\"role\":\"user\", \n",
    "                                                \"content\": response_doctor})\n",
    "\n",
    "            # Doctor arrives at a differential diagnosis\n",
    "            if (\"?\" not in response_doctor) or ('final diagnosis' in response_doctor.lower()):\n",
    "\n",
    "                # multi-turn conversation + 4-choice MCQs\n",
    "                c = conversation_history_doctor[:-1]\n",
    "                c.append({\"role\": \"system\", \"content\": mcq_prompt})\n",
    "\n",
    "                mcq = mapping[gpt_model](c, n_responses=1)\n",
    "                \n",
    "                # multi-turn conversation + many-choice MCQs\n",
    "                c = conversation_history_doctor[:-1]\n",
    "                c.append({\"role\": \"system\", \"content\": mcq_all_prompt})\n",
    "\n",
    "                mcq_all = mapping[gpt_model](c, n_responses=1)\n",
    "\n",
    "                break\n",
    "\n",
    "        stats[case_id][f'trial_{j}_doctor_responses'] = conversation_history_doctor[:]\n",
    "        stats[case_id][f'trial_{j}_mcq'] = mcq\n",
    "        stats[case_id][f'trial_{j}_mcq_many'] = mcq_all\n",
    "        j += 1\n",
    "        \n",
    "        json.dump(stats, open(save_path,\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648404eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_gpt3 = \"./results/conversations_raw/conversations_gpt3.json\"\n",
    "path_gpt4 = \"./results/conversations_raw/conversations_gpt4.json\"\n",
    "\n",
    "for case in tqdm(cases):\n",
    "    process_case_withoutPE(case, path_gpt3, \"gpt-3.5\", num_runs=10)\n",
    "    process_case_withoutPE(case, path_gpt4, \"gpt-4\", num_runs=10)\n",
    "    process_case_withPE(case, path_gpt3, \"gpt-3.5\", num_runs=10)    \n",
    "    process_case_withPE(case, path_gpt4, \"gpt-4\", num_runs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39512f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
